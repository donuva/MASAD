{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6892971,"sourceType":"datasetVersion","datasetId":3959805},{"sourceId":6931831,"sourceType":"datasetVersion","datasetId":3980288},{"sourceId":6953093,"sourceType":"datasetVersion","datasetId":3993555}],"dockerImageVersionId":30580,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# TASK = AE, AP\n\nTASK = 'AP'\nprint(TASK)\n\nreload = True","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:53:51.368467Z","iopub.execute_input":"2023-11-20T13:53:51.369121Z","iopub.status.idle":"2023-11-20T13:53:51.380642Z","shell.execute_reply.started":"2023-11-20T13:53:51.369069Z","shell.execute_reply":"2023-11-20T13:53:51.379619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install lightning\n!pip install fasttext\n!pip install comet_ml","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2023-11-20T13:53:51.382129Z","iopub.execute_input":"2023-11-20T13:53:51.382397Z","iopub.status.idle":"2023-11-20T13:54:36.166912Z","shell.execute_reply.started":"2023-11-20T13:53:51.382371Z","shell.execute_reply":"2023-11-20T13:54:36.165711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pickle\nimport pandas as pd\nimport json\nimport os\nimport ast\nimport math\nimport re\nimport copy","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:54:36.168997Z","iopub.execute_input":"2023-11-20T13:54:36.169358Z","iopub.status.idle":"2023-11-20T13:54:36.543242Z","shell.execute_reply.started":"2023-11-20T13:54:36.169324Z","shell.execute_reply":"2023-11-20T13:54:36.542316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import ToTensor\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nCUDA_LAUNCH_BLOCKING=1","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:54:36.544410Z","iopub.execute_input":"2023-11-20T13:54:36.544888Z","iopub.status.idle":"2023-11-20T13:54:39.883024Z","shell.execute_reply.started":"2023-11-20T13:54:36.544854Z","shell.execute_reply":"2023-11-20T13:54:39.882144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# metric\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport comet_ml\n\ncomet_ml.init(project_name=\"MASAD\")\n#RDaPLQIi9AFz0Eaeu2PEQg0X","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:54:39.885634Z","iopub.execute_input":"2023-11-20T13:54:39.886205Z","iopub.status.idle":"2023-11-20T13:55:21.643281Z","shell.execute_reply.started":"2023-11-20T13:54:39.886167Z","shell.execute_reply":"2023-11-20T13:55:21.642382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightning as L\nfrom pytorch_lightning.loggers import CometLogger\nimport pytorch_lightning as pl\n\ncomet_logger = CometLogger()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:55:21.644485Z","iopub.execute_input":"2023-11-20T13:55:21.644826Z","iopub.status.idle":"2023-11-20T13:55:25.272191Z","shell.execute_reply.started":"2023-11-20T13:55:21.644799Z","shell.execute_reply":"2023-11-20T13:55:25.271378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import FastText\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:55:25.273541Z","iopub.execute_input":"2023-11-20T13:55:25.274076Z","iopub.status.idle":"2023-11-20T13:55:47.157731Z","shell.execute_reply.started":"2023-11-20T13:55:25.274048Z","shell.execute_reply":"2023-11-20T13:55:47.156717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aspect = {'spider': 0,   'road': 1,    'dance': 2,     'milk': 3,       'horse': 4, \n          'cloud': 5,    'glass': 6,   'insect': 7,    'flower': 8,     'eye': 9, \n          'apple': 10,   'dog': 11,    'river': 12,    'leave': 13,     'church': 14, \n          'winter': 15,  'bird': 16,   'factory': 17,  'night': 18,     'car': 19, \n          'coast': 20,   'teen': 21,   'face': 22,     'house': 23,     'tree': 24, \n          'boat': 25,    'castle': 26, 'mushroom': 27, 'bady': 28,      'beach': 29, \n          'fish': 30,    'socks': 31,  'street': 32,   'girl': 33,      'chair': 34, \n          'feet': 35,    'autumn': 36, 'food': 37,     'forest': 38,    'cat': 39, \n          'rose': 40,    'band': 41,   'train': 42,    'chocolate': 43, 'grandfather': 44, \n          'boy': 45,     'plant': 46,  'ocean': 47,    'waterfall': 48, 'toy': 49, \n          'cupcake': 50, 'hat': 51,    'tshirt': 52,   'meat': 53,      'graden': 54, \n          'market': 55,  'cake': 56}\n\npolarity = {\n    'positive': 0,\n    'negative': 1\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:55:47.158955Z","iopub.execute_input":"2023-11-20T13:55:47.159493Z","iopub.status.idle":"2023-11-20T13:55:47.168327Z","shell.execute_reply.started":"2023-11-20T13:55:47.159465Z","shell.execute_reply":"2023-11-20T13:55:47.167348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config_img = {\n    'image_size': 224,\n    'patch_size': 16,\n    'num_channels': 3,\n    'num_heads': 12,\n    'num_layers': 12,\n    'input_dim': 2048,\n    'hidden_dim': 768,\n    'output_dim': 1000,\n}\n\nconfig_text = {\n    'input_dim': 300,\n    'hidden_dim':  300, \n    'embedding': 300,\n    'num_layers': 5,\n    'output_dim': 1000,\n    'num_channels': 3,\n    'num_heads': 12,#embedding phai chia het cho num_heads\n}\n\nBATCH_SIZE=32\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:55:47.169658Z","iopub.execute_input":"2023-11-20T13:55:47.170371Z","iopub.status.idle":"2023-11-20T13:55:47.192489Z","shell.execute_reply.started":"2023-11-20T13:55:47.170339Z","shell.execute_reply":"2023-11-20T13:55:47.191390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evalution(labels, predicts):\n    accuracy = accuracy_score(labels, predicts)\n    f1 = f1_score(labels, predicts, average='weighted', zero_division = 0)\n    recall = recall_score(labels, predicts, average='weighted', zero_division = 0)\n    precision = precision_score(labels, predicts, average='weighted', zero_division = 0)\n\n    accuracy = round(accuracy * 100, 4)\n    f1 = round(f1 * 100, 4)\n    recall = round(recall * 100, 4)\n    precision = round(precision * 100, 4)\n\n    return accuracy, f1, precision, recall","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:55:47.193984Z","iopub.execute_input":"2023-11-20T13:55:47.194369Z","iopub.status.idle":"2023-11-20T13:55:47.203726Z","shell.execute_reply.started":"2023-11-20T13:55:47.194337Z","shell.execute_reply":"2023-11-20T13:55:47.202843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_file(file_path):\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    \n    return data\n\ndata_raw_train = read_file('/kaggle/input/masadtext/train.json')\ndata_raw_test = read_file('/kaggle/input/masadtext/test.json')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-20T13:55:47.208337Z","iopub.execute_input":"2023-11-20T13:55:47.208636Z","iopub.status.idle":"2023-11-20T13:57:44.617766Z","shell.execute_reply.started":"2023-11-20T13:55:47.208603Z","shell.execute_reply":"2023-11-20T13:57:44.616937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_file = '/kaggle/working/text.model'\n\nif os.path.isfile(output_file):\n    print(f\"File {output_file} already exists. Aborting.\")\nelse:\n    sentences = []\n    \n    all_data = data_raw_train\n    for d in all_data:\n        sentences.append(re.sub(r'\\([^)]*\\)', '', ' '.join(d['token'])).split())\n\n    all_data = data_raw_test\n    for d in all_data:\n        sentences.append(re.sub(r'\\([^)]*\\)', '', ' '.join(d['token'])).split())\n\n    model_ted = FastText(sentences, vector_size=300, window=5, min_count=1, workers=4, sg=1)\n    \n    word_vectors = model_ted.wv\n\n    model_ted.save(output_file)\n    \ntext_model = FastText.load(\"text.model\")\ntext_model = text_model.wv\n\nprint(len(text_model.index_to_key))","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:57:44.618830Z","iopub.execute_input":"2023-11-20T13:57:44.619135Z","iopub.status.idle":"2023-11-20T13:58:33.291493Z","shell.execute_reply.started":"2023-11-20T13:57:44.619108Z","shell.execute_reply":"2023-11-20T13:58:33.290466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_dict = {'<unk>': 0 }\nfor word in text_model.index_to_key:\n    word_dict[word] = len(word_dict)\n\nembedding = []\nembedding.append(np.zeros(300))\n\nfor key in word_dict.keys():\n    embedding.append(text_model[key])\n    \nembedding = np.array(embedding)\nembedding = torch.Tensor(embedding).to(device)\n\nprint(word_dict)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-20T13:58:33.292763Z","iopub.execute_input":"2023-11-20T13:58:33.293096Z","iopub.status.idle":"2023-11-20T13:58:36.919244Z","shell.execute_reply.started":"2023-11-20T13:58:33.293068Z","shell.execute_reply":"2023-11-20T13:58:36.918137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_file = '/kaggle/working/aspect.model'\n\nif os.path.isfile(output_file):\n    print(f\"File {output_file} already exists. Aborting.\")\nelse:\n    sentences = []\n\n    for key in aspect.keys():\n        sentences.append(key)\n\n    model_ted = FastText(sentences, vector_size=300, window=5, min_count=1, workers=4, sg=1)\n\n    word_vectors = model_ted.wv\n\n    model_ted.save(output_file)\n    \naspect_model = FastText.load(\"aspect.model\")\n\naspect_weight = []\nfor key in aspect.keys():\n    x = aspect_model.wv[key]\n    x = np.array([x] * 300) \n    x = (x + x.T) / 2\n    \n    aspect_weight.append(x)\n    \naspect_weight = torch.Tensor(aspect_weight).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:58:36.920459Z","iopub.execute_input":"2023-11-20T13:58:36.920748Z","iopub.status.idle":"2023-11-20T13:58:50.343670Z","shell.execute_reply.started":"2023-11-20T13:58:36.920723Z","shell.execute_reply":"2023-11-20T13:58:50.342669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p = {\n    '<unk>': 0\n}\nall_data = data_raw_train\nfor d in all_data:\n    for i in d['pos']:\n        if i not in p:\n            p[i] = len(p)\n\nall_data = data_raw_test\nfor d in all_data:\n    for i in d['pos']:\n        if i not in p:\n            p[i] = len(p)\n\nprint(p)\n\nnum_pos = len(p)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:58:50.345044Z","iopub.execute_input":"2023-11-20T13:58:50.345350Z","iopub.status.idle":"2023-11-20T13:58:50.503054Z","shell.execute_reply.started":"2023-11-20T13:58:50.345324Z","shell.execute_reply":"2023-11-20T13:58:50.502149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_file = '/kaggle/working/tag.model'\n\nif os.path.isfile(output_file):\n    print(f\"File {output_file} already exists. Aborting.\")\nelse:\n    sentences = []\n    \n    all_data = data_raw_train\n    for d in all_data:\n        for i in d['tag']:\n            sentences.append(i)\n\n    all_data = data_raw_test\n    for d in all_data:\n        for i in d['tag']:\n            sentences.append(i)\n\n    model_ted = FastText(sentences, vector_size=300, window=5, min_count=1, workers=4, sg=1)\n\n    word_vectors = model_ted.wv\n\n    model_ted.save(output_file)\n\ntag_model = FastText.load(\"tag.model\")\ntag_model = tag_model.wv","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:58:50.504356Z","iopub.execute_input":"2023-11-20T13:58:50.504654Z","iopub.status.idle":"2023-11-20T13:59:22.356149Z","shell.execute_reply.started":"2023-11-20T13:58:50.504628Z","shell.execute_reply":"2023-11-20T13:59:22.355337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class datasets(Dataset):\n    def __init__(self, data):\n        self.data = data\n        self.transforms = transforms.Compose([\n            transforms.Resize((config_img['image_size'], config_img['image_size'])),\n            transforms.ToTensor()\n        ])\n            \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        # text\n        token = []\n        n = self.data[idx]['token']\n        for i in range(config_text['embedding']):\n            if (i < len(n) and n[i] in word_dict):\n                token.append(word_dict[n[i]])\n            else:\n                token.append(0)\n        token = torch.Tensor(token).to(device)\n        \n        #aspect\n        asp = self.data[idx]['aspect']\n        asp_ids = aspect[asp]\n        \n        if(TASK == 'AP'):\n            label = polarity[self.data[idx]['polarity']]\n        else:\n            label = aspect[self.data[idx]['aspect']]\n        \n        # image\n        img_path = self.data[idx]['imgpath']\n        try:\n            image = Image.open(img_path).convert('RGB')\n        except Exception as e:\n            image = Image.new(\"RGB\", (config_img['image_size'], config_img['image_size']), (0, 0, 0))\n            \n        image = self.transforms(image)\n        \n        # tag\n        list_tag =  self.data[idx]['tag']\n        vector_tag = []\n        \n        for i in range(50):\n            if(i < len(list_tag)):\n                vector_tag.append(tag_model[list_tag[i]])\n            else:\n                vector_tag.append(np.zeros(300))\n        \n        vector_tag = torch.tensor(vector_tag)\n        \n        #onehot\n        vector_onehot = torch.zeros(len(aspect))\n        vector_onehot[aspect[asp]] = 1\n        \n        #short\n        mask_0 = [[-99999] * config_text['embedding'] for _ in range(config_text['embedding'])]\n        mask_1 = [[-99999] * config_text['embedding'] for _ in range(config_text['embedding'])]\n        mask_2 = [[-99999] * config_text['embedding'] for _ in range(config_text['embedding'])]\n        mask_3 = [[-99999] * config_text['embedding'] for _ in range(config_text['embedding'])]\n        mask_4 = [[-99999] * config_text['embedding'] for _ in range(config_text['embedding'])]\n        short_length = min(len(self.data[idx]['short']), config_text['embedding'])\n        for i in range(short_length):\n            for j in range(short_length):\n                mask_0[i][j] = 0\n                if self.data[idx]['short'][i][j] == 1:\n                    mask_1[i][j] = 0 \n                    mask_2[i][j] = 0\n                    mask_3[i][j] = 0\n                    mask_4[i][j] = 0\n                elif self.data[idx]['short'][i][j] == 2:\n                    mask_2[i][j] = 0\n                    mask_3[i][j] = 0\n                    mask_4[i][j] = 0\n                elif self.data[idx]['short'][i][j] == 3:\n                    mask_3[i][j] = 0\n                    mask_4[i][j] = 0\n                elif self.data[idx]['short'][i][j] == 4:\n                    mask_4[i][j] = 0\n\n        for i in range(short_length):\n            mask_1[i][i] = 0 \n            mask_2[i][i] = 0\n            mask_3[i][i] = 0\n            mask_4[i][i] = 0\n        \n        short_mask = []\n        short_mask.append(mask_0)\n        short_mask.append(mask_1)\n        short_mask.append(mask_2)\n        short_mask.append(mask_3)\n        short_mask.append(mask_4)\n        \n        short_mask = torch.Tensor(short_mask).to(device)\n            \n#         pos \n#         pos = []\n#         n = self.data[idx]['pos']\n#         for i in range(config_text['embedding']):\n#             if (i < len(n)):\n#                 pos.append(p[n[i]])\n#             else:\n#                 pos.append(0)\n                \n#         pos = torch.Tensor(pos).to(device)\n        \n        output = {\n            \"token\": token,\n            'aspect': asp,\n            'img': image,\n            'label': label,\n            'asp_ids': asp_ids,\n            'onehot': vector_onehot,\n            'tag': vector_tag,\n            'short': short_mask,\n            'pos': 1\n        }\n        \n        return output\n\nreload = True","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:59:22.357491Z","iopub.execute_input":"2023-11-20T13:59:22.357808Z","iopub.status.idle":"2023-11-20T13:59:22.390736Z","shell.execute_reply.started":"2023-11-20T13:59:22.357780Z","shell.execute_reply":"2023-11-20T13:59:22.389919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(reload)\n\ndef load_data(file_path, data_raw, reload = True):\n    data = []\n    \n    if reload:\n        data = datasets(data_raw)\n        torch.save(data, file_path)\n        return data\n    else:\n        data = torch.load(file_path, map_location=device)  \n        return data \n\ndata_train = load_data('/kaggle/working/data_train.pth', data_raw_train, reload)\ndata_test = load_data('/kaggle/working/data_test.pth', data_raw_test, reload)\n\nprint(len(data_train))\nprint(len(data_test))\n    \nreload = False","metadata":{"execution":{"iopub.status.busy":"2023-11-20T13:59:22.391948Z","iopub.execute_input":"2023-11-20T13:59:22.392288Z","iopub.status.idle":"2023-11-20T14:02:09.095568Z","shell.execute_reply.started":"2023-11-20T13:59:22.392261Z","shell.execute_reply":"2023-11-20T14:02:09.094617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(data_train, batch_size=BATCH_SIZE, shuffle = True)\ntest_dataloader = DataLoader(data_test, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T14:02:09.096848Z","iopub.execute_input":"2023-11-20T14:02:09.097185Z","iopub.status.idle":"2023-11-20T14:02:09.102208Z","shell.execute_reply.started":"2023-11-20T14:02:09.097158Z","shell.execute_reply":"2023-11-20T14:02:09.101151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_args(batch):\n    img = batch['img']\n    token = batch['token']\n    short = batch['short']\n    labels = batch['label']\n#     asp_ids = batch['asp_ids']\n    ls_tag = batch['tag']\n    onehot = batch['onehot']\n\n    img = img.float().to(device) \n    ls_tag = ls_tag.float().to(device)\n    token = token.long().to(device)\n    labels = labels.long().to(device)\n    onehot = onehot.float().to(device)\n    short = short.float().to(device)\n\n    return img, ls_tag, token, short, onehot, labels","metadata":{"execution":{"iopub.status.busy":"2023-11-20T14:02:09.103455Z","iopub.execute_input":"2023-11-20T14:02:09.103736Z","iopub.status.idle":"2023-11-20T14:02:09.117630Z","shell.execute_reply.started":"2023-11-20T14:02:09.103712Z","shell.execute_reply":"2023-11-20T14:02:09.116602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rnn_zero_state(batch_size, hidden_dim, num_layers, bidirectional=True): \n    total_layers = num_layers * 2 if bidirectional else num_layers\n    state_shape = (total_layers, batch_size, hidden_dim)   \n    h0 = c0 = Variable(torch.zeros(*state_shape), requires_grad=False)\n    return h0.to(device), c0.to(device)\n\ndef clones(module, N): # Tạo N bản sao của một module.\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\ndef self_attention(query):\n    d_k = query.size(-1) \n    scores = torch.matmul(query, query.transpose(-2, -1))\n    scores = scores / math.sqrt(d_k) \n    return scores\n\ndef aspect_attention(batch, weight_m, key, bias_m):\n    p=weight_m.size(0)\n    mx=weight_m.size(1)\n    weight_m=weight_m.unsqueeze(0).expand(batch, p, mx, mx).to(device)\n    \n    batch, x, y, z = key.shape\n    aspect = torch.ones((batch, x, y, weight_m.size()[2])).to(device)\n    H_W = torch.matmul(aspect, weight_m).to(device)\n    \n    aspect_scores = torch.matmul(H_W, key.transpose(-2, -1)).to(device)\n    aspect_scores = torch.add(aspect_scores, bias_m).to(device)\n    aspect_scores = torch.tanh(aspect_scores).to(device)\n    \n    return aspect_scores\n\ndef syntax_mask(scores, short):\n    scores = torch.add(scores, short)\n    scores = F.softmax(scores, dim=-1)\n    return scores","metadata":{"execution":{"iopub.status.busy":"2023-11-20T14:02:09.118733Z","iopub.execute_input":"2023-11-20T14:02:09.119025Z","iopub.status.idle":"2023-11-20T14:02:09.130067Z","shell.execute_reply.started":"2023-11-20T14:02:09.119002Z","shell.execute_reply":"2023-11-20T14:02:09.129239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self):\n        super(Attention, self).__init__()\n        self.h = 5\n        self.hidden_size = config_text['hidden_dim'] * 2 # sửa cho size của out phù hợp\n        self.d_k = self.hidden_size // self.h\n        \n        self.linears = clones(nn.Linear(self.hidden_size, self.hidden_size), 2)\n        self.weight_m = nn.Parameter(torch.Tensor(self.h, self.hidden_size // self.h, self.hidden_size // self.h))\n        self.bias_m = nn.Parameter(torch.Tensor(1))\n        \n    def forward(self, query, short):\n        batch = query.size(0)\n        key = query\n        query, key = [l(x).view(batch, -1, self.h, self.d_k).transpose(1, 2)  \n                             for l, x in zip(self.linears, (query, key))]  \n        scores = self_attention(query)\n        \n        if 'AP' in TASK:\n            scores_aspect = aspect_attention(batch, self.weight_m, key, self.bias_m)\n            scores = torch.add(scores, scores_aspect)\n        \n        scores = syntax_mask(scores, short)\n        \n        return scores\n\nclass GCN(nn.Module):\n    def __init__(self, layer = 3, output_dim = 1000):\n        super(GCN, self).__init__()\n        self.layer = layer\n        self.attention_heads = 5\n        self.hidden_size = config_text['hidden_dim']\n        self.embedding = config_text['embedding']\n        \n        self.W = nn.Linear(self.embedding, self.embedding) \n        \n        self.gcn_drop = nn.Dropout(0.1)\n        self.fc1 = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.fc2 = nn.Linear(self.hidden_size * self.hidden_size, self.hidden_size * 10)\n        self.fc3 = nn.Linear(self.hidden_size * 10, output_dim)\n        \n    def forward(self, inp, weight_adj): \n        out = inp\n        batch = inp.shape[0]\n        for i in range(self.layer):\n            out = out.unsqueeze(1).expand(batch, self.attention_heads, self.embedding, self.hidden_size * 2)\n            out = torch.matmul(weight_adj, out).mean(dim=1)\n            out = F.selu(out)\n            out = self.gcn_drop(out) if i < self.layer - 1 else out \n        \n        out=self.fc1(out)\n        out = out.reshape(out.shape[0],  -1)\n        out=self.fc2(out)\n        out=self.fc3(out)\n        out=F.relu(out)\n        return out\n\nclass ModelText(nn.Module):\n    def __init__(self):\n        super(ModelText, self).__init__()\n        self.hidden_size = config_text['hidden_dim']\n        self.num_layers = config_text['num_layers']\n        self.input_size = config_text['input_dim']\n        self.embedding = config_text['embedding']\n        self.h = 5\n        self.embs = nn.Embedding.from_pretrained(torch.tensor(embedding, dtype=torch.float), freeze=True)\n        self.pos_embs = nn.Embedding(num_pos, self.embedding, dtype=torch.float,padding_idx=0)\n        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True, bidirectional=True)\n        self.attention = Attention().to(device)\n        self.gcn = GCN().to(device)\n        self.dropout = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(1000, len(aspect))\n            \n    def encode_with_rnn(self, rnn_inputs):\n        batch_size = len(rnn_inputs)\n        h0, c0 = rnn_zero_state(batch_size, self.hidden_size, self.num_layers) \n        rnn_outputs, (ht, ct) = self.lstm(rnn_inputs, (h0, c0))\n        return rnn_outputs\n    \n    def forward(self, x, short):\n        x = self.embs(x)\n        out = self.encode_with_rnn(x)\n#         print(x.shape)\n#         print(out.shape)\n        short.zero_()\n        gcn_input = self.attention(out, short)\n        out = self.gcn(out, gcn_input)\n        # đẩu ra gcn là 1000\n        out = self.fc(out)\n        return out.squeeze(1)\n    \nmodel_text = ModelText().to(device)\n#print(model_text)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T14:02:09.131722Z","iopub.execute_input":"2023-11-20T14:02:09.132143Z","iopub.status.idle":"2023-11-20T14:02:13.880524Z","shell.execute_reply.started":"2023-11-20T14:02:09.132108Z","shell.execute_reply":"2023-11-20T14:02:13.879623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Bottleneck(nn.Module):\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes = 1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        \n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, 1000)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\ndef resnet152():\n    model = ResNet(Bottleneck, [3, 4, 6, 3]).to(device)\n    model.load_state_dict(torch.load('/kaggle/input/resnet-pretrain/resnet50-19c8e357.pth', map_location='cpu'))\n    return model.to(device)\n\nclass myResnet(nn.Module):\n    def __init__(self):\n        super(myResnet, self).__init__()\n        self.resnet = resnet152().to(device)\n        \n        self.noun_linear=nn.Linear(300, config_img['hidden_dim'])\n        self.multi_linear=nn.Linear(config_img['input_dim'], config_img['hidden_dim'])\n        self.att_linear=nn.Linear(config_img['hidden_dim']*2, 1)\n        self.linear=nn.Linear(config_img['hidden_dim']*2, 1)\n        self.alpha_linear1=nn.Linear(config_img['input_dim'], config_img['hidden_dim'])\n        self.alpha_linear2=nn.Linear(config_img['input_dim'], config_img['hidden_dim'])\n        self.fc = nn.Linear(300, config_img['input_dim'])\n        self.fc1 = nn.Linear(1000 + config_img['input_dim'] * 7 * 7, len(aspect))\n        \n        self.tag_att = self.noun_attention\n    \n    def noun_attention(self, encoder_outputs, noun_embed):\n        encoder_outputs = encoder_outputs.float().to(device)\n        noun_embed = noun_embed.float().to(device)\n        multi_features_rep = encoder_outputs.unsqueeze(2).repeat(1, 1, noun_embed.shape[1], 1)\n        noun_features_rep = noun_embed.unsqueeze(1).repeat(1, encoder_outputs.shape[1], 1, 1)\n        noun_features_rep = self.noun_linear(noun_features_rep)\n        multi_features_rep = self.multi_linear(multi_features_rep)\n        concat_features = torch.tanh(torch.cat([noun_features_rep, multi_features_rep], dim=-1))\n        att = torch.softmax(self.att_linear(concat_features).squeeze(-1), dim=-1)\n        att_features = self.fc(torch.matmul(att, noun_embed))\n        alpha = torch.cat([self.alpha_linear1(encoder_outputs), self.alpha_linear2(att_features)], dim=-1)\n        alpha = torch.sigmoid(self.linear(alpha))\n        alpha = alpha.repeat(1, 1, 1)\n\n        encoder_outputs = torch.mul(1 - alpha, encoder_outputs)\n        encoder_outputs += torch.mul(alpha, att_features)\n        return encoder_outputs\n\n    def forward(self, x, list_tag = None, att_size=7):\n        x = self.resnet.conv1(x)\n        x = self.resnet.bn1(x)\n        x = self.resnet.relu(x)\n        x = self.resnet.maxpool(x)\n\n        x = self.resnet.layer1(x)\n        x = self.resnet.layer2(x)\n        x = self.resnet.layer3(x)\n        x = self.resnet.layer4(x)\n\n        fc = x.mean(3).mean(2)\n        att = F.adaptive_avg_pool2d(x,[att_size,att_size])\n        att = att.view(-1, 2048, 49).permute(0, 2, 1)\n        \n        list_tag = list_tag.double()\n        \n        abc = self.tag_att(encoder_outputs=att, noun_embed=list_tag)\n\n        x = self.resnet.avgpool(x)\n        x = x.view(x.size(0), -1)\n        abc = abc.reshape(abc.size(0), -1)\n        \n        x = self.resnet.fc(x)\n        \n        x = torch.cat((x, abc), dim = 1)\n        x = self.fc1(x)\n\n        return x\n    \nmodel_img = myResnet().to(device)\nprint(model_img)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-20T14:02:13.882063Z","iopub.execute_input":"2023-11-20T14:02:13.882460Z","iopub.status.idle":"2023-11-20T14:02:15.633226Z","shell.execute_reply.started":"2023-11-20T14:02:13.882425Z","shell.execute_reply":"2023-11-20T14:02:15.632276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class model_full(pl.LightningModule):\n    def __init__(self):\n        super(model_full, self).__init__()\n        \n        self.text = model_text\n        self.img = model_img\n        \n        if 'AP' in TASK:\n            self.fc = nn.Sequential(\n                nn.Linear((3 * len(aspect)), len(aspect)),\n                nn.Linear((len(aspect)), 2)\n            )\n            \n        if 'AE' in TASK:\n            self.fc = nn.Linear((2 * len(aspect)), len(aspect))\n            \n        self.criterion = nn.CrossEntropyLoss()\n        \n        self.test_preds = []\n        self.test_labels = []\n        self.matrix = []\n        \n        #comel\n        self.save_hyperparameters()\n        \n    def forward(self, img, list_tag, text,short, onehot):\n        text = self.text(text, short)\n        img = self.img(img, list_tag)\n        \n        outputs = torch.cat((text, img), dim=1)\n        \n        if 'AP' in TASK:\n            outputs = torch.cat((outputs, onehot), dim=1)\n        \n        outputs = self.fc(outputs)\n        \n        return outputs\n#      img, ls_tag, token, short, onehot, labels\n    \n    def training_step(self, batch, batch_idx):\n        img, ls_tag, token, short, onehot, labels = process_args(batch)\n        outputs = self(img, ls_tag, token, short, onehot)\n        loss = self.criterion(outputs, labels)\n        self.log('loss', loss, on_step=True, on_epoch=False, prog_bar=True, logger=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        img, ls_tag, token, short, onehot, labels = process_args(batch)\n        outputs = self(img, ls_tag, token, short, onehot)\n        \n        predicts = torch.argmax(outputs, dim=1)\n        labels = labels.cpu().numpy()\n        predicts = predicts.cpu().numpy()\n        \n        self.test_preds.extend(predicts)\n        self.test_labels.extend(labels)\n        \n        acc, f1, prec, rec = evalution(labels, predicts)\n        \n        return {'val_acc': acc, 'val_f1': f1, 'val_prec': prec, 'val_rec': rec}\n\n    def test_step(self, batch, batch_idx):\n        img, ls_tag, token, short, onehot, labels = process_args(batch)\n        outputs = self(img, ls_tag, token, short, onehot)\n        \n        predicts = torch.argmax(outputs, dim=1)\n        labels = labels.cpu().numpy()\n        predicts = predicts.cpu().numpy()\n        \n        self.test_preds.extend(predicts)\n        self.test_labels.extend(labels)\n        \n        acc, f1, prec, rec = evalution(labels, predicts)\n        \n        return {'test_acc': acc, 'test_f1': f1, 'test_prec': prec, 'test_rec': rec}\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.00001)\n        self.logger.experiment.log_metric('lr', optimizer.param_groups[0]['lr'])\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_acc'}\n    \n    def on_validation_epoch_end(self):\n        acc, f1, prec, rec = evalution(self.test_labels, self.test_preds)\n        self.write(inp = {'val_acc': acc, 'val_f1': f1, 'val_prec': prec, 'val_rec': rec})\n        self.test_labels = []\n        self.test_preds = []\n        \n    def on_test_epoch_end(self):\n        acc, f1, prec, rec = evalution(self.test_labels, self.test_preds)\n        self.write(inp = {'val_acc': acc, 'val_f1': f1, 'val_prec': prec, 'val_rec': rec})\n        self.test_labels = []\n        self.test_preds = []\n    \n    def write(self, inp):\n        for key in inp.keys():\n            self.log(key, inp[key], on_epoch=True, prog_bar=True, logger=True)\n    \n    def getDomain(self):\n        return self.matrix\n    \n    def getLabelTest(self):\n        return self.test_labels\n    \n    def getLabelPred(self):\n        return self.test_preds\n\n\nmodel = model_full().to(device)\nprint(model)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-20T14:02:15.634747Z","iopub.execute_input":"2023-11-20T14:02:15.635029Z","iopub.status.idle":"2023-11-20T14:02:15.783939Z","shell.execute_reply.started":"2023-11-20T14:02:15.635006Z","shell.execute_reply":"2023-11-20T14:02:15.783029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_EPOCHS = 1\ncomet_logger.log_hyperparams({\"batch_size\": BATCH_SIZE})\n\naccelerator = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntrainer = pl.Trainer(max_epochs=NUM_EPOCHS, accelerator=accelerator, logger=comet_logger)\ntrainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\ntrainer.test(model, dataloaders=test_dataloader)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-20T14:02:15.785130Z","iopub.execute_input":"2023-11-20T14:02:15.785423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = model.getLabelPred()\ntest_labels = model.getLabelTest()\nprint(\"Test Classification Report:\\n\", classification_report(test_labels, test_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix = confusion_matrix(test_labels, test_preds)\n\nplt.figure(figsize=(8, 6))\nax = sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=True)\n\ntick_labels = [] \n\nif TASK == 'AE':\n    for key in aspect.keys():\n        tick_labels.append(key)\nelse:\n    for key in polarity.keys():\n        tick_labels.append(key)\n    \n    \nax.set_xticklabels(tick_labels, rotation=0)\nax.set_yticklabels(tick_labels, rotation=0)\n\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"food = {\n    \"apple\": 0, \n    \"food\": 0, \n    \"milk\": 0, \n    \"mushroom\": 0,\n    \"chocolate\":0,\n    \"meat\":0,\n    \"cake\":0\n} # 6\n\ngoods = {\n    \"boat\": 0, \n    \"car\": 0, \n    \"chair\": 0, \n    \"glass\": 0, \n    \"socks\": 0, \n    \"train\": 0,\n    \"cupcake\":0,\n    \"hat\":0,\n    \"toy\":0,\n    \"tshirt\":0\n}#10\n\nbuildings = {\n    \"castle\": 0,\n    \"church\": 0,\n    \"factory\": 0,\n    \"house\": 0,\n    \"road\": 0,\n    \"market\":0\n}#6\n\nanimal = {\n    \"bird\": 0,\n    \"cat\" : 0,\n    \"dog\": 0,\n    \"fish\": 0,\n    \"horse\": 0,\n    \"insect\": 0,\n    \"spider\": 0\n}#7\n\nhuman = {\n    \"bady\": 0, # masad viết sai chính tả 'baby'\n    \"band\": 0,\n    \"dance\": 0, \n    \"eye\": 0, \n    \"face\": 0, \n    \"feet\": 0, \n    \"girl\": 0, \n    \"teen\": 0,\n    \"boy\":0,\n    \"grandfather\":0\n}#10\n\nplant = {\n    \"flower\": 0,\n    \"rose\": 0,\n    \"tree\": 0, \n    \"leave\": 0,\n    \"plant\":0\n}#5\n\nscenery = {\n    \"autumn\": 0,\n    \"beach\": 0,\n    \"cloud\": 0,\n    \"coast\": 0,\n    \"night\": 0,\n    \"river\": 0,\n    \"street\": 0,\n    \"winter\": 0,\n    \"graden\":0, #garden is wrong write\n    \"ocean\":0,\n    \"waterfall\":0,\n    \"forest\":0\n} #12","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix =  model.getDomain()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"food_true = 0\ntotal_food = 0\n\ngoods_true = 0\ntotal_goods = 0\n\nbuildings_true = 0\ntotal_builddings = 0\n\nanimal_true = 0\ntotal_animal = 0\n\nhuman_true = 0\ntotal_human = 0\n\nplant_true = 0\ntotal_plant = 0\n\nscenery_true = 0\ntotal_scenery = 0\n\nfor asp, predict in matrix:\n    if asp in food:\n        food_true += (predict == True)\n        total_food += 1\n        \n    elif asp in goods:\n        goods_true += (predict == True)\n        total_goods += 1\n        \n    elif asp in buildings:\n        buildings_true += (predict == True)\n        total_builddings += 1\n        \n    elif asp in animal:\n        animal_true += (predict == True)\n        total_animal += 1\n        \n    elif asp in human:\n        human_true += (predict == True)\n        total_human += 1\n        \n    elif asp in plant:\n        plant_true += (predict == True)\n        total_plant += 1\n    elif asp in scenery:\n        scenery_true += (predict == True)\n        total_scenery += 1\n    else:\n        print(asp, predict)\n        \nx = 0\n        \naccuracy = 100 * scenery_true / (total_scenery + 1e-10)\nprint(f'Accuracy on test domain scenery: {accuracy}%')\n\nx += accuracy\n\naccuracy = 100 * food_true / (total_food+ 1e-10)\nprint(f'Accuracy on test domain food: {accuracy}%')\n\nx += accuracy\n\naccuracy = 100 * goods_true / (total_goods+ 1e-10)\nprint(f'Accuracy on test domain goods: {accuracy}%')\n\nx += accuracy\n\naccuracy = 100 * buildings_true / (total_builddings+ 1e-10)\nprint(f'Accuracy on test domain buildings: {accuracy}%')\n\nx += accuracy\n\naccuracy = 100 * animal_true / (total_animal+ 1e-10)\nprint(f'Accuracy on test domain animal: {accuracy}%')\n\nx += accuracy\n\naccuracy = 100 * human_true / (total_human+ 1e-10)\nprint(f'Accuracy on test domain human: {accuracy}%')\n\nx += accuracy\n\naccuracy = 100 * plant_true / (total_plant+ 1e-10)\nprint(f'Accuracy on test domain plant: {accuracy}%')\n\nx += accuracy\n\nprint('-------------------')\nprint(x/7)\n\nprint(scenery_true + food_true + goods_true + buildings_true + animal_true + human_true + plant_true)\nprint(total_scenery + total_food + total_goods + total_builddings + total_animal + total_human + total_plant)\n\nprint('-------------------')\nx = scenery_true + food_true + goods_true + buildings_true + animal_true + human_true + plant_true\ny = total_scenery + total_food + total_goods + total_builddings + total_animal + total_human + total_plant\n\nprint(x/y)\n\nprint('-------------------')\nx = scenery_true/total_scenery + food_true/total_food + goods_true/total_goods + buildings_true/total_builddings + animal_true/total_animal + human_true/total_human + plant_true/total_plant\nprint(x/7)\n\nprint('-------------------')\nprint(scenery_true/total_scenery)\nprint(food_true/total_food)\nprint(goods_true/total_goods)\nprint(buildings_true/total_builddings)\nprint(animal_true/total_animal)\nprint(human_true/total_human)\nprint(plant_true/total_plant)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}